Multi-Stage Progressive Image Restoration
Syed Waqas Zamir* 1 Aditya Arora* 1 Salman Khan2 Munawar Hayat3
Fahad Shahbaz Khan2 Ming-Hsuan Yang4,5,6 Ling Shao1,2
1Inception Institute of AI 2Mohamed bin Zayed University of AI 3Monash University
4University of California, Merced 5Yonsei University 6Google Research
Abstract
Image restoration tasks demand a complex balance between spatial details and high-level contextualized information while recovering images. In this paper, we propose a
novel synergistic design that can optimally balance these
competing goals. Our main proposal is a multi-stage architecture, that progressively learns restoration functions
for the degraded inputs, thereby breaking down the overall recovery process into more manageable steps. Specifically, our model first learns the contextualized features using encoder-decoder architectures and later combines them
with a high-resolution branch that retains local information. At each stage, we introduce a novel per-pixel adaptive design that leverages in-situ supervised attention to
reweight the local features. A key ingredient in such a
multi-stage architecture is the information exchange between different stages. To this end, we propose a twofaceted approach where the information is not only exchanged sequentially from early to late stages, but lateral
connections between feature processing blocks also exist to
avoid any loss of information. The resulting tightly interlinked multi-stage architecture, named as MPRNet, delivers
strong performance gains on ten datasets across a range
of tasks including image deraining, deblurring, and denoising. The source code and pre-trained models are available
at https://github.com/swz30/MPRNet.
1. Introduction
Image restoration is the task of recovering a clean image
from its degraded version. Typical examples of degradation
include noise, blur, rain, haze, etc. It is a highly ill-posed
problem as there exist infinite feasible solutions. In order to
restrict the solution space to valid/natural images, existing
restoration techniques [19, 29, 39, 59, 66, 67, 100] explicitly use image priors that are handcrafted with empirical observations. However, designing such priors is a challenging
task and often not generalizable. To ameliorate this issue,
recent state-of-the-art approaches [17, 44, 57, 86, 87, 93, 94,
97] employ convolutional neural networks (CNNs) that im-
*Equal contribution4 6 8 10 12 14 16 18 20 22
Number of parameters (Millions)
29.0
29.5
30.0
30.5
31.0
31.5
32.0
32.5
PSNR (dB)
Baseline
MPRNet (ours)
Nah
CVPR17
SRN
CVPR18
DMPHN
CVPR19
Suin
CVPR20DBGAN
CVPR20
Zhang
CVPR18
[70]
[53]
[92]
[88]
[91]
[71]
Figure 1: Image deblurring on the GoPro dataset [53]. Under
different parameter capacities (x-axis), our multi-stage approach
performs better than the single-stage baseline [65] (with channel
attention [95]), as well as the state-of-the-art (PSNR on y-axis).
plicitly learn more general priors by capturing natural image
statistics from large-scale data.
The performance gain of CNN-based methods over the
others is primarily attributed to its model design. Numerous network modules and functional units for image
restoration have been developed including recursive residual learning [4, 95], dilated convolutions [4, 81], attention
mechanisms [17, 86, 96], dense connections [73, 75, 97],
encoder-decoders [7, 13, 43, 65], and generative models [44, 62, 90, 92]. Nevertheless, nearly all of these models for low-level vision problems are based on single-stage
design. In contrast, multi-stage networks are shown to be
more effective than their single-stage counterparts in highlevel vision problems such as pose-estimation [14, 46, 54],
scene parsing [15] and action segmentation [20, 26, 45].
Recently, few efforts have been made to bring the multistage design to image deblurring [70, 71, 88], and image
deraining [47, 63]. We analyze these approaches to identify the architectural bottlenecks that hamper their performance. First, existing multi-stage techniques either employ
the encoder-decoder architecture [71, 88] which is effective in encoding broad contextual information but unreliable
in preserving spatial image details, or use a single-scale
pipeline [63] that provides spatially accurate but semanti-
arXiv:2102.02808v2 [cs.CV] 16 Mar 2021cally less reliable outputs. However, we show that the combination of both design choices in a multi-stage architecture
is needed for effective image restoration. Second, we show
that naively passing the output of one stage to the next stage
yields suboptimal results [53]. Third, unlike in [88], it is important to provide ground-truth supervision at each stage for
progressive restoration. Finally, during multi-stage processing, a mechanism to propagate intermediate features from
earlier to later stages is required to preserve contextualized
features from the encoder-decoder branches.
We propose a multi-stage progressive image restoration
architecture, called MPRNet, with several key components.
1). The earlier stages employ an encoder-decoder for learning multi-scale contextual information, while the last stage
operates on the original image resolution to preserve fine
spatial details. 2). A supervised attention module (SAM)
is plugged between every two stages to enable progressive
learning. With the guidance of ground-truth image, this
module exploits the previous stage prediction to compute
attention maps that are in turn used to refine the previous
stage features before being passed to the next stage. 3). A
mechanism of cross-stage feature fusion (CSFF) is added
that helps propagating multi-scale contextualized features
from the earlier to later stages. Furthermore, this method
eases the information flow among stages, which is effective
in stabilizing the multi-stage network optimization.
The main contributions of this work are:
• A novel multi-stage approach capable of generating
contextually-enriched and spatially accurate outputs. Due
to its multi-stage nature, our framework breaks down the
challenging image restoration task into sub-tasks to progressively restore a degraded image.
• An effective supervised attention module that takes full
advantage of the restored image at every stage in refining
incoming features before propagating them further.
• A strategy to aggregate multi-scale features across stages.
• We demonstrate the effectiveness of our MPRNet by setting new state-of-the-art on ten synthetic and real-world
datasets for various restoration tasks including image deraining, deblurring, and denoising while maintaining a
low complexity (see Fig. 1). Further, we provide detailed
ablations, qualitative results, and generalization tests.
2. Related Work
Recent years have witnessed a paradigm shift from highend DSLR cameras to smartphone cameras. However,
capturing high-quality images with smartphone cameras is
challenging. Image degradations are often present in images either due to the limitations of cameras and/or adverse
ambient conditions. Early restoration approaches are based
on total variation [10, 67], sparse coding [3, 51, 52], selfsimilarity [8, 16], gradient prior [68, 80], etc. Recently,
CNN-based restoration methods have achieved state-of-theart results [57, 70, 86, 93, 97]. In terms of architectural
design, these methods can be broadly categorized as singlestage and multi-stage.
Single-Stage Approaches. Currently, the majority of image restoration methods are based on a single-stage design, and the architectural components are usually based on
those developed for high-level vision tasks. For example,
residual learning [30] has been used to perform image denoising [2, 72, 93], image deblurring [42, 43] and image
deraining [37]. Similarly, to extract multi-scale information, the encoder-decoder [65] and dilated convolution [83]
models are often used [4, 28, 43]. Other single-stage approaches [5, 89, 97] incorporate dense connections [34].
Multi-Stage Approaches. These methods [24, 47, 53, 63,
70, 71, 88, 99] aim to recover clean image in a progressive
manner by employing a light-weight subnetwork at each
stage. Such a design is effective since it decomposes the
challenging image restoration task into smaller easier subtasks. However, a common practice is to use the identical
subnetwork for each stage which may yield suboptimal results, as shown in our experiments (Section 4).
Attention. Driven by its success in high-level tasks such
as image classification [31, 32, 79], segmentation [21, 35]
and detection [74, 79], attention modules have been used
in low-level vision tasks [38]. Examples abound, including
methods for image deraining [37, 47], deblurring [61, 70],
super-resolution [17, 95], and denoising [4, 86]. The main
idea is to capture long-range inter-dependencies along spatial dimensions [98], channel dimensions [32], or both [79].
3. Multi-Stage Progressive Restoration
The proposed framework for image restoration, shown in
Fig. 2, consists of three stages to progressively restore images. The first two stages are based on encoder-decoder
subnetworks that learn the broad contextual information
due to large receptive fields. Since image restoration is a
position-sensitive task (which requires pixel-to-pixel correspondence from the input to output), the last stage employs
a subnetwork that operates on the original input image resolution (without any downsampling operation), thereby preserving the desired fine texture in the final output image.
Instead of simply cascading multiple stages, we incorporate a supervised attention module between every two
stages. With the supervision of ground-truth images, our
module rescales the feature maps of the previous stage before passing them to the next stage. Furthermore, we introduce a cross-stage feature fusion mechanism where the
intermediate multi-scale contextualized features of the earlier subnetwork help consolidating the intermediate features
of the latter subnetwork.
Although MPRNet stacks multiple stages, each stage hasFigure 2: Proposed multi-stage architecture for progressive image restoration. Earlier stages employ encoder-decoders to extract
multi-scale contextualized features, while the last stage operates
at the original image resolution to generate spatially accurate outputs. A supervised attention module is added between every two
stages that learns to refine features of one stage before passing
them to the next stage. Dotted pink arrows represent the crossstage feature fusion mechanism.
an access to the input image. Similar to the recent restoration methods [70, 88], we adapt the multi-patch hierarchy
on the input image and split the image into non-overlapping
patches: four for stage-1, two for stage-2, and the original
image for the last stage, as shown in Fig. 2.
At any given stage S, instead of directly predicting a restored image XS , the proposed model predicts a residual
image RS to which the degraded input image I is added to
obtain: XS = I + RS . We optimize our MPRNet end-toend with the following loss function:
L =
3∑
S=1
[Lchar (XS , Y) + λLedge(XS , Y)] , (1)
where Y represents the ground-truth image, and Lchar is
the Charbonnier loss [12]:
Lchar =
√
‖XS − Y‖2 + ε2, (2)
with constant ε empirically set to 10−3 for all the experiments. In addition, Ledge is the edge loss, defined as:
Ledge =
√
‖∆(XS ) − ∆(Y)‖2 + ε2, (3)
where ∆ denotes the Laplacian operator. The parameter λ
in Eq. (1) controls the relative importance of the two loss
terms, which is set to 0.05 as in [37]. Next, we describe
each key element of our method.
3.1. Complementary Feature Processing
Existing single-stage CNNs for image restoration typically use one of the following architecture designs: 1). An
encoder-decoder, or 2). A single-scale feature pipeline. The
encoder-decoder networks [7, 13, 43, 65] first gradually
map the input to low-resolution representations, and then
progressively apply reverse mapping to recover the original
resolution. While these models effectively encode multiscale information, they are prone to sacrificing spatial details due to the repeated use of downsampling operation. In
contrast, the approaches that operate on single-scale feature
pipeline are reliable in generating images with fine spatial
details [6, 18, 93, 97]. However, their outputs are semantically less robust due to the limited receptive field. This
indicates the inherent limitations of the aforementioned architecture design choices that are capable of generating either spatially accurate or contextually reliable outputs, but
not both. To exploit the merits of both designs, we propose
a multi-stage framework where earlier stages incorporate
the encoder-decoder networks, and the final stage employs
a network that operates on the original input resolution.
Encoder-Decoder Subnetwork. Figure 3a shows our
encoder-decoder subnetwork, which is based on the standard U-Net [65], with the following components. First, we
add channel attention blocks (CABs) [95] to extract features
at each scale (See Fig. 3b for CABs). Second, the feature
maps at U-Net skip connections are also processed with the
CAB. Finally, instead of using Transposed convolution for
increasing spatial resolution of features in the decoder, we
use bilinear upsampling followed by a convolution layer.
This helps reduce checkerboard artifacts in the output image that often arise due to the Transposed convolution [55].
Original Resolution Subnetwork. In order to preserve
fine details from the input image to the output image, we
introduce the original-resolution subnetwork (ORSNet) in
the last stage (see Fig. 2). ORSNet does not employ any
downsampling operation and generates spatially-enriched
high-resolution features. It consists of multiple originalresolution blocks (ORBs), each of which further contains
CABs. The schematic of ORB is illustrated in Fig. 3b.
3.2. Cross-stage Feature Fusion
In our framework, we introduce the CSFF module between two encoder-decoders (see Fig. 3c), and between
encoder-decoder and ORSNet (see Fig. 3d). Note that the
features from one stage are first refined with 1 × 1 convolutions before propagating them to the next stage for aggregation. The proposed CSFF has several merits. First, it makes
the network less vulnerable by the information loss due to
repeated use of up- and down-sampling operations in the
encoder-decoder. Second, the multi-scale features of one
stage help enriching the features of the next stage. Third,
the network optimization procedure becomes more stable
as it eases the flow of information, thereby allowing us to
add several stages in the overall architecture.(a) (b) (c) (d)
Figure 3: (a) Encoder-decoder subnetwork. (b) Illustration of the original resolution block (ORB) in our ORSNet subnetwork. Each ORB
contains multiple channel attention blocks. GAP represents global average pooling [49]. (c) Cross-stage feature fusion between stage 1
and stage 2. (d) CSFF between stage 2 and the last stage.
Figure 4: Supervised attention module.
3.3. Supervised Attention Module
Recent multi-stage networks for image restoration [70,
88] directly predict an image at each stage, which is then
passed to the next consecutive stage. Instead, we introduce
a supervised attention module between every two stages,
which facilitates achieving significant performance gain.
The schematic diagram of SAM is shown in Fig. 4, and
its contributions are two-fold. First, it provides groundtruth supervisory signals useful for the progressive image
restoration at each stage. Second, with the help of locally
supervised predictions, we generate attention maps to suppress the less informative features at the current stage and
only allow the useful ones to propagate to the next stage.
As illustrated in Fig. 4, SAM takes the incoming features
Fin ∈ RH×W ×C of the earlier stage and first generates a
residual image RS ∈ RH×W ×3 with a simple 1 × 1 convolution, where H × W denotes the spatial dimension and
C is the number of channels. The residual image is added
to the degraded input image I to obtain the restored image
XS ∈ RH×W ×3. To this predicted image XS , we provide
explicit supervision with the ground-truth image. Next, perpixel attention masks M ∈ RH×W ×C are generated from
the image XS using a 1×1 convolution followed by the sigmoid activation. These masks are then used to re-calibrate
the transformed local features Fin (obtained after 1×1 convolution), resulting in attention-guided features which are
added to the identity mapping path. Finally, the attentionaugmented feature representation Fout, produced by SAM,
is passed to the next stage for further processing.
4. Experiments and Analysis
We evaluate our method for several image restoration
tasks, including (a) image deraining, (b) image deblurring,
and (c) image denoising on 10 different datasets.
4.1. Datasets and Evaluation Protocol
Quantitative comparisons are performed using the PSNR
and SSIM [76] metrics. As in [7], we report (in parenthesis) the reduction in error for each method relative to
the best performing method by translating PSNR to RMSE
(RMSE ∝ √10−PSNR/10) and SSIM to DSSIM (DSSIM =
(1 − SSIM)/2). The datasets used for training and testing
are summarized in Table 1 and described next.
Image Deraining. Using the same experimental setups of
the recent best method on image deraining [37], we train
our model on 13,712 clean-rain image pairs gathered from
multiple datasets [23, 48, 81, 89, 90], as shown in Table 1.
With this single trained model, we perform evaluation on
various test sets, including Rain100H [81], Rain100L [81],
Test100 [90], Test2800 [23], and Test1200 [89].
Image Deblurring. As in [70, 88, 43, 71], we use the GoPro [53] dataset that contains 2,103 image pairs for training and 1,111 pairs for evaluation. Furthermore, to demonstrate generalizability, we take our GoPro trained model
and directly apply it on the test images of the HIDE [69]
and RealBlur [64] datasets. The HIDE dataset is specifically collected for human-aware motion deblurring and its
test set contains 2,025 images. While the GoPro and HIDE
datasets are synthetically generated, the image pairs of RealBlur dataset are captured in real-world conditions. The
RealBlur dataset has two subsets: (1) RealBlur-J is formed
with the camera JPEG outputs, and (2) RealBlur-R is generated offline by applying white balance, demosaicking, and
denoising operations to the RAW images.
Image Denoising. To train our model for image denoising task, we use 320 high-resolution images of the SIDD
dataset [1]. Evaluation is conducted on 1,280 validation
patches from the SIDD dataset [1] and 1,000 patches from
the DND benchmark dataset [60]. These test patches are
extracted from the full resolution images by the original au-Table 1: Dataset description for various image restoration tasks.
Tasks Deraining Deblurring Denoising
Datasets Rain14000 [23] Rain1800 [81] Rain800 [90] Rain100H [81] Rain100L [81] Rain1200 [89] Rain12 [48] GoPro [53] HIDE [69] RealBlur [64] SIDD [1] DND [60]
Train Samples 11200 1800 700 0 0 0 12 2103 0 0 320 0
Test Samples 2800 0 100 100 100 1200 0 1111 2025 1960 40 50
Testset Rename Test2800 - Test100 Rain100H Rain100L Test1200 - - - - - -
Table 2: Image deraining results. Best and second best scores are highlighted and underlined. For each method, reduction in error relative
to the best-performing algorithm is reported in parenthesis (see Section 4.1 for error calculation technique). Our MPRNet achieves ∼20%
relative improvement in PSNR over the previous best method MSPFN [37].
Test100 [90] Rain100H [81] Rain100L [81] Test2800 [23] Test1200 [89] Average
Methods PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑
DerainNet [22] 22.77 0.810 14.92 0.592 27.03 0.884 24.31 0.861 23.38 0.835 22.48 (69.3%) 0.796 (61.3%)
SEMI [77] 22.35 0.788 16.56 0.486 25.03 0.842 24.43 0.782 26.05 0.822 22.88 (67.8%) 0.744 (69.1%)
DIDMDN [89] 22.56 0.818 17.35 0.524 25.23 0.741 28.13 0.867 29.65 0.901 24.58 (60.9%) 0.770 (65.7%)
UMRL [82] 24.41 0.829 26.01 0.832 29.18 0.923 29.97 0.905 30.55 0.910 28.02 (41.9%) 0.880 (34.2%)
RESCAN [47] 25.00 0.835 26.36 0.786 29.80 0.881 31.29 0.904 30.51 0.882 28.59 (37.9%) 0.857 (44.8%)
PreNet [63] 24.81 0.851 26.77 0.858 32.44 0.950 31.75 0.916 31.36 0.911 29.42 (31.7%) 0.897 (23.3%)
MSPFN [37] 27.50 0.876 28.66 0.860 32.40 0.933 32.82 0.930 32.39 0.916 30.75 (20.4%) 0.903 (18.6%)
MPRNet (Ours) 30.27 0.897 30.41 0.890 36.40 0.965 33.64 0.938 32.91 0.916 32.73 (0.0%) 0.921 (0.0%)
thors. Both SIDD and DND datasets consist of real images.
4.2. Implementation Details
Our MPRNet is end-to-end trainable and requires no pretraining. We train separate models for three different tasks.
We employ 2 CABs at each scale of the encoder-decoder,
and for downsampling we use 2×2 max-pooling with stride
2. In the last stage, we employ ORSNet that contains 3
ORBs, each of which further uses 8 CABs. Depending on
the task complexity, we scale the network width by setting
the number of channels to 40 for deraining, 80 for denoising, and 96 for deblurring. The networks are trained on
256×256 patches with a batch size of 16 for 4×105 iterations. For data augmentation, horizontal and vertical flips
are randomly applied. We use Adam optimizer [41] with the
initial learning rate of 2×10−4, which is steadily decreased
to 1×10−6 using the cosine annealing strategy [50].
4.3. Image Deraining Results
For the image deraining task, consistent with prior
work [37], we compute image quality scores using the
Y channel (in YCbCr color space). Table 2 shows
that our method significantly advances state-of-the-art by
consistently achieving better PSNR/SSIM scores on all
five datasets. Compared to the recent best algorithm
MSPFN [37], we obtain a performance gain of 1.98 dB (average across all datasets), indicating 20% error reduction.
The improvements on some datasets are as large as 4 dB,
e.g., Rain100L [81]. Further, our model has 3.7× fewer
parameters than MSPFN [37], while being 2.4× faster.
Figure 5 shows visual comparisons on challenging images. Our MPRNet is effective in removing rain streaks
of different orientations and magnitudes, and generates images that are visually pleasant and faithful to the groundtruth. In contrast, other approaches compromise structural
content (first row), introduce artifacts (second row), and do
not completely remove rain streaks (third row).
4.4. Image Deblurring Results
We report the performance of evaluated image deblurring approaches on the synthetic GoPro [53] and HIDE [69]
datasets in Table 3. Overall, our model performs favorably
against other algorithms. Compared to the previous best
performing technique [70], our method achieves 9% improvement in PSNR and 21% in SSIM on the GoPro [53]
dataset, and a 11% and 13% reduction in error on the HIDE
dataset [69]. It is worth noticing that our network is trained
only on the GoPro dataset, but achieves the state-of-the-art
results (+0.98 dB) on the HIDE dataset, thereby demonstrating its strong generalization capability.
We evaluate our MPRNet on the real-world images of
a recent RealBlur [64] dataset under two experimental settings: 1). apply the GoPro trained model directly on RealBlur (to test generalization to real images), and 2). train
and test on RealBlur data. Table 4 shows the experimental results. For setting 1, our MPRNet obtains performance
gains of 0.29 dB on the RealBlur-R subset and 0.28 dB
on the RealBlur-J subset over the DMPHN algorithm [88].
A similar trend is observed for setting 2, where our gains
over SRN [71] are 0.66 dB and 0.38 dB on RealBlur-R and
RealBlur-J, respectively.
Figure 6 shows some deblurred images by the evaluated
approaches. Overall, the images restored by our model are
sharper and closer to the ground-truth than those by others.PSNR 18.76 dB 20.23 dB 23.36 dB 23.66 dB
Reference Rainy DerainNet [22] DIDMDN [89] SEMI [77]
18.76 dB 25.52 dB 26.88 dB 27.16 dB 29.86 dB 32.15 dB
Rainy Image UMRL [82] RESCAN [47] PreNet [63] MSPFN [37] MPRNet (Ours)
PSNR 11.04 dB 14.70 dB 13.01 dB 27.15 dB 26.55 dB 28.67 dB 30.62 dB
PSNR 22.51 dB 21.94 dB 23.35 dB 25.21 dB 25.84 dB 25.04 dB 38.08 dB
Reference Rainy DIDMDN [89] SEMI [77] UMRL [82] RESCAN [47] MSPFN [37] MPRNet (Ours)
Figure 5: Image deraining results. Our MPRNet effectively removes rain and generates images that are natural, artifact-free and visually
closer to the ground-truth.
Table 3: Deblurring results. Our method is trained only on the
GoPro dataset [53] and directly applied to the HIDE dataset [69].
GoPro [53] HIDE [69]
Method PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑
Xu et al. [80] 21.00 (73.9%) 0.741 (84.2%) - -
Hyun et al. [36] 23.64 (64.6%) 0.824 (76.7%) - -
Whyte et al. [78] 24.60 (60.5%) 0.846 (73.4%) - -
Gong et al. [27] 26.40 (51.4%) 0.863 (70.1%) - -
DeblurGAN [42] 28.70 (36.6%) 0.858 (71.1%) 24.51 (52.4%) 0.871 (52.7%)
Nah et al. [53] 29.08 (33.8%) 0.914 (52.3%) 25.73 (45.2%) 0.874 (51.6%)
Zhang et al. [91] 29.19 (32.9%) 0.931 (40.6%) - -
DeblurGAN-v2 [43] 29.55 (30.1%) 0.934 (37.9%) 26.61 (39.4%) 0.875 (51.2%)
SRN [71] 30.26 (24.1%) 0.934 (37.9%) 28.36 (25.9%) 0.915 (28.2%)
Shen et al. [69] - - 28.89 (21.2%) 0.930 (12.9%)
Gao et al. [25] 30.90 (18.3%) 0.935 (36.9%) 29.11 (19.2%) 0.913 (29.9%)
DBGAN [92] 31.10 (16.4%) 0.942 (29.3%) 28.94 (20.8%) 0.915 (28.2%)
MT-RNN [58] 31.15 (16.0%) 0.945 (25.5%) 29.15 (18.8%) 0.918 (25.6%)
DMPHN [88] 31.20 (15.5%) 0.940 (31.7%) 29.09 (19.4%) 0.924 (19.7%)
Suin et al. [70] 31.85 (8.9%) 0.948 (21.2%) 29.98 (10.7%) 0.930 (12.9%)
MPRNet (Ours) 32.66 (0.0%) 0.959 (0.0%) 30.96 (0.0%) 0.939 (0.0%)
4.5. Image Denoising Results
In Table 5, we report PSNR/SSIM scores of several image denoising methods on the SIDD [1] and DND [60]
datasets. Our method obtains considerable gains over the
state-of-the-art approaches, i.e., 0.19 dB over CycleISP [86]
on SIDD and 0.21 dB over SADNet [11] on DND. Note that
the DND dataset does not contain any training images, i.e.,
the complete publicly released dataset is just a test set. ExTable 4: Deblurring comparisons on the RealBlur dataset [64] under two different settings: 1). applying our GoPro trained model
directly on the RealBlur set (to evaluate generalization to real images), 2). Training and testing on RealBlur data where methods are
denoted with symbol ‡. The PSNR/SSIM scores for other evaluated approaches are taken from the RealBlur benchmark [64].
RealBlur-R RealBlur-J
Method PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑
Hu et al. [33] 33.67 (23.4%) 0.916 (42.9%) 26.41 (23.2%) 0.803 (35.5%)
Nah et al. [53] 32.51 (33.0%) 0.841 (69.8%) 27.87 (9.1%) 0.827 (26.6%)
DeblurGAN [42] 33.79 (22.4%) 0.903 (50.5%) 27.97 (8.1%) 0.834 (23.5%)
Pan et al. [56] 34.01 (20.4%) 0.916 (42.9%) 27.22 (15.7%) 0.790 (39.5%)
Xu et al. [80] 34.46 (16.2%) 0.937 (23.8%) 27.14 (16.4%) 0.830 (25.3%)
DeblurGAN-v2 [43] 35.26 (8.1%) 0.944 (14.3%) 28.70 (0.0%) 0.866 (5.2%)
Zhang et al. [91] 35.48 (5.7%) 0.947 (9.4%) 27.80 (9.8%) 0.847 (17.0%)
SRN [71] 35.66 (3.7%) 0.947 (9.4%) 28.56 (1.6%) 0.867 (4.5%)
DMPHN [88] 35.70 (3.3%) 0.948 (7.7%) 28.42 (3.2%) 0.860 (9.3%)
MPRNet (Ours) 35.99 (0.0%) 0.952 (0.0%) 28.70 (0.0%) 0.873 (0.0%)
‡DeblurGAN-v2 [43] 36.44 (28.1%) 0.935 (56.9%) 29.69 (21.2%) 0.870 (40.0%)
‡SRN [71] 38.65 (7.3%) 0.965 (20.0%) 31.38 (4.3%) 0.909 (14.3%)
‡MPRNet (Ours) 39.31 (0.0%) 0.972 (0.0%) 31.76 (0.0%) 0.922 (0.0%)
perimental results on the DND benchmark with our SIDD
trained model demonstrates our model generalizes well to
different image domains.
Fig. 7 illustrates visual results. Our method is able to
remove real noise, while preserving the structural and textural image details. In contrast, the images restored by other
methods contain either overly smooth contents, or artifacts
with splotchy textures.PSNR 24.19 dB 27.42 dB 28.68 dB 27.78 dB
Reference Blurry SRN [71] DeblurGANv2 [43] Gao et al. [25]
24.19 dB 28.60 dB 28.46 dB 28.54 dB 28.80 dB 29.16 dB
Blurry Image DBGAN [92] MTRNN [58] DMPHN [88] Suin et al. [70] MPRNet (Ours)
PSNR 19.59 dB 24.91 dB 26.30 dB 26.20 dB
Reference Blurry SRN [71] DeblurGANv2 [43] Gao et al. [25]
19.59 dB 26.15 dB 25.02 dB 26.66 dB 26.95 dB 28.68 dB
Blurry Image DBGAN [92] MTRNN [58] DMPHN [88] Suin et al. [70] MPRNet (Ours)
Figure 6: Visual comparisons for image deblurring on the GoPro datatset [53]. Compared to the state-of-the-art methods, our MPRNet
restores more sharper and perceptually-faithful images.
Table 5: Denoising comparisons on SIDD [1] and DND [60]
datasets. ∗ denotes the methods that use additional training data.
Whereas our MPRNet is only trained on the SIDD images and directly tested on DND.
SIDD [1] DND [60]
Method PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑
DnCNN [93] 23.66 (84.2%) 0.583 (89.9%) 32.43 (57.2%) 0.790 (79.1%)
MLP [9] 24.71 (82.2%) 0.641 (88.3%) 34.23 (47.3%) 0.833 (73.7%)
BM3D [16] 25.65 (80.2%) 0.685 (86.7%) 34.51 (45.6%) 0.851 (70.5%)
CBDNet* [28] 30.78 (64.2%) 0.801 (78.9%) 38.06 (18.2%) 0.942 (24.1%)
RIDNet* [4] 38.71 (10.9%) 0.951 (14.3%) 39.26 (6.0%) 0.953 (6.4%)
AINDNet* [40] 38.95 (8.4%) 0.952 (12.5%) 39.37 (4.8%) 0.951 (10.2%)
VDN [84] 39.28 (4.8%) 0.956 (4.6%) 39.38 (4.7%) 0.952 (8.3%)
SADNet* [11] 39.46 (2.8%) 0.957 (2.3%) 39.59 (2.4%) 0.952 (8.3%)
DANet+* [85] 39.47 (2.7%) 0.957 (2.3%) 39.58 (2.5%) 0.955 (2.2%)
CycleISP* [86] 39.52 (2.2%) 0.957 (2.3%) 39.56 (2.7%) 0.956 (0.0%)
MPRNet (Ours) 39.71 (0.0%) 0.958 (0.0%) 39.80 (0.0%) 0.954 (4.4%)
4.6. Ablation Studies
Here we present ablation experiments to analyze the contribution of each component of our model. Evaluation is
performed on the GoPro dataset [53] with the deblurring
models trained on image patches of size 128×128 for 105
iterations, and the results are shown in Table 6.
Number of stages. Our model yields better performance as
the number of stages increases, which validates the effectiveness of our multi-stage design.
Choices of subnetworks. Since each stage of our model
could employ different subnetwork design, we test different
options. We show that using the encoder-decoder in the earlier stage(s) and the ORSNet in the last stage leads to imTable 6: Ablation study on individual components of the
proposed MPRNet.
#Stages Stage Combination SAM CSFF PSNR
1 U-Net (baseline) - - 28.94
1 ORSNet (baseline) - - 28.91
2 U-Net + U-Net 7 7 29.40
2 ORSNet + ORSNet 7 7 29.53
2 U-Net + ORSNet 7 7 29.70
3 U-Nets + ORSNet 7 7 29.86
3 U-Nets + ORSNet 7 3 30.07
3 U-Nets + ORSNet 3 7 30.31
3 U-Nets + ORSNet 3 3 30.49
proved performance (29.7 dB) as compared to employing
the same design for all the stages (29.4 dB with U-Net+UNet, and 29.53 dB with ORSNet+ORSNet).
SAM and CSFF. We demonstrate the effectiveness of the
proposed supervised attention module and cross-stage feature fusion mechanism by removing them from our final
model. Table 6 shows a substantial drop in PSNR from
30.49 dB to 30.07 dB when SAM is removed, and from
30.49 dB to 30.31 dB when we take out CSFF. Removing
both of these components degrades the performance by a
large margin from 30.49 dB to 29.86 dB.
5. Resource Efficient Image Restoration
CNN models generally exhibit a trade-off between accuracy and computational efficiency. In the pursuit of achieving higher accuracy, deeper and complex models are often
developed. Although large models tend to perform better
than their smaller counterparts, the computational cost can26.90 dB 30.91 dB 33.62 dB 33.89 dB 34.09 dB
Noisy BM3D [16] CBDNet [28] VDN [84] RIDNet [4]
26.90 dB 34.32 dB 34.36 dB 34.36 dB 34.52 dB 34.91 dB
Noisy Image CycleISP [86] AINDNet [40] DANet [85] SADNet [11] MPRNet (Ours)
PSNR 18.25 dB 35.57 dB 36.24 dB 36.39 dB 36.70 dB 36.71 dB 36.74 dB 36.98 dB
PSNR 18.16 dB 29.83 dB 29.99 dB 30.31 dB 30.48 dB 30.22 dB 30.76 dB 31.17 dB
Reference Noisy RIDNet [4] AINDNet [40] VDN [84] SADNet [11] CycleISP [86] DANet [85] MPRNet (Ours)
Figure 7: Image denoising comparisons. First example is from DND [60] and the others from SIDD [1]. The proposed MPRNet better
preserves fine texture and structural patterns in the denoised images.
Table 7: Stage-wise deblurring performance of MPRNet on GoPro [53]. Runtimes are computed with the Nvidia Titan Xp GPU.
Method DeblurGAN-v2 SRN DMPHN Suin MPRNet (ours)
[43] [71] [88] et al. [70] 1-stage 2-stages 3-stages
PSNR 29.55 30.10 31.20 31.85 30.43 31.81 32.66
#Params (M) 60.9 6.8 21.7 23.0 5.6 11.3 20.1
Time (s) 0.21 0.57 1.07 0.34 0.04 0.08 0.18
be prohibitively high. As such, it is of great interest to develop resource-efficient image restoration models. One solution is to train the same network by adjusting its capacity every time the target system is changed. However, it
is tedious and oftentimes infeasible. A more desirable approach is to have a single network that can make (a) early
predictions for compute efficient systems and (b) latter predictions to obtain high accuracy. A multi-stage restoration
model naturally offers such functionalities.
Table 7 reports the stage-wise results of our multi-stage
approach. Our MPRNet demonstrates competitive restoration performance at each stage. Notably, our stage-1 model
is light, fast, and yields better results than other sophisticated algorithms such as SRN [71] and DeblurGANv2 [43]. Similarly, when compared to a recent method DMPHN [88], our stage-2 model shows the PSNR gain of 0.51
dB while being more resource-efficient (∼2× fewer parameters and 13× faster).
6. Conclusion
In this work, we propose a multi-stage architecture for
image restoration that progressively improves degraded inputs by injecting supervision at each stage. We develop
guiding principles for our design that demand complementary feature processing in multiple stages and a flexible information exchange between them. To this end, we propose
contextually-enriched and spatially accurate stages that encode a diverse set of features in unison. To ensure synergy
between reciprocal stages, we propose feature fusion across
stages and an attention guided output exchange from earlier stages to the later ones. Our model achieves significant
performance gains on numerous benchmark datasets. In addition, our model is light-weighted in terms of model size
and efficient in terms of runtime, which are of great interest
for devices with limited resources.
Acknowledgments. M.-H. Yang is supported in part by the
NSF CAREER Grant 1149783. Special thanks to Kui Jiang
for providing image deraining results.